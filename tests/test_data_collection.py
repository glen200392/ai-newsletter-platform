"""
æ¸¬è©¦å¸‚å ´æ•¸æ“šæ”¶é›†èˆ‡æ•´åˆ
æ¸¬è©¦ RSS feeds å’Œ Public APIs çš„æ•¸æ“šæŠ“å–èƒ½åŠ›
"""

import asyncio
from datetime import datetime, timedelta
from typing import List, Dict
import json

# ============================================================================
# æ¨¡æ“¬æ•¸æ“šæºï¼ˆçœŸå¯¦ç’°å¢ƒæœƒèª¿ç”¨å¯¦éš› APIï¼‰
# ============================================================================

class MockDataSource:
    """æ¨¡æ“¬æ•¸æ“šæº"""
    
    @staticmethod
    def fetch_rss_feed(source_name: str) -> List[Dict]:
        """æ¨¡æ“¬ RSS Feed æŠ“å–"""
        
        mock_data = {
            "TechCrunch": [
                {
                    "title": "OpenAI Announces GPT-5 with Multimodal Capabilities",
                    "url": "https://techcrunch.com/2026/02/04/openai-gpt5",
                    "published": datetime.now() - timedelta(hours=2),
                    "summary": "OpenAI unveils GPT-5, featuring advanced multimodal processing and improved reasoning capabilities.",
                    "source": "TechCrunch",
                    "category": "AI/ML"
                },
                {
                    "title": "Stripe Valuation Hits $100B After Latest Funding Round",
                    "url": "https://techcrunch.com/2026/02/03/stripe-funding",
                    "published": datetime.now() - timedelta(hours=12),
                    "summary": "Payment processor Stripe reaches $100B valuation in Series H funding round.",
                    "source": "TechCrunch",
                    "category": "Fintech"
                }
            ],
            "Bloomberg": [
                {
                    "title": "Fed Signals Potential Rate Cut in Q2 2026",
                    "url": "https://bloomberg.com/news/fed-rate-cut",
                    "published": datetime.now() - timedelta(hours=5),
                    "summary": "Federal Reserve chair hints at possible interest rate reduction amid cooling inflation.",
                    "source": "Bloomberg",
                    "category": "Economics"
                },
                {
                    "title": "Tech Stocks Rally on Strong Earnings Reports",
                    "url": "https://bloomberg.com/markets/tech-rally",
                    "published": datetime.now() - timedelta(hours=8),
                    "summary": "Major tech companies exceed Q4 expectations, driving market surge.",
                    "source": "Bloomberg",
                    "category": "Markets"
                }
            ],
            "Wired": [
                {
                    "title": "Quantum Computing Breakthrough: IBM Achieves Error Correction Milestone",
                    "url": "https://wired.com/quantum-breakthrough",
                    "published": datetime.now() - timedelta(hours=6),
                    "summary": "IBM's new quantum processor demonstrates practical error correction at scale.",
                    "source": "Wired",
                    "category": "Technology"
                }
            ],
            "Harvard Business Review": [
                {
                    "title": "The Future of Remote Work: Hybrid Models That Actually Work",
                    "url": "https://hbr.org/remote-work-future",
                    "published": datetime.now() - timedelta(days=1),
                    "summary": "Research reveals which hybrid work models drive productivity and employee satisfaction.",
                    "source": "Harvard Business Review",
                    "category": "Management"
                },
                {
                    "title": "CEO Decision-Making in the Age of AI",
                    "url": "https://hbr.org/ceo-ai-decisions",
                    "published": datetime.now() - timedelta(days=2),
                    "summary": "How top executives are leveraging AI to improve strategic decision-making.",
                    "source": "Harvard Business Review",
                    "category": "Leadership"
                }
            ]
        }
        
        return mock_data.get(source_name, [])
    
    @staticmethod
    def fetch_hacker_news() -> List[Dict]:
        """æ¨¡æ“¬ Hacker News API"""
        return [
            {
                "title": "Show HN: I built an AI coding assistant that actually understands context",
                "url": "https://news.ycombinator.com/item?id=123456",
                "published": datetime.now() - timedelta(hours=3),
                "score": 450,
                "comments": 120,
                "source": "Hacker News",
                "category": "Developer Tools"
            },
            {
                "title": "Why We're Migrating 100M Users from PostgreSQL to CockroachDB",
                "url": "https://news.ycombinator.com/item?id=123457",
                "published": datetime.now() - timedelta(hours=7),
                "score": 380,
                "comments": 95,
                "source": "Hacker News",
                "category": "Databases"
            }
        ]
    
    @staticmethod
    def fetch_arxiv_papers(topic: str) -> List[Dict]:
        """æ¨¡æ“¬ arXiv API"""
        papers = {
            "AI": [
                {
                    "title": "Scaling Laws for Large Language Models: New Insights",
                    "authors": ["Smith, J.", "Chen, L.", "Garcia, M."],
                    "url": "https://arxiv.org/abs/2602.12345",
                    "published": datetime.now() - timedelta(days=1),
                    "summary": "Novel analysis of scaling behavior in transformer-based models.",
                    "source": "arXiv",
                    "category": "AI Research"
                }
            ],
            "Quantum": [
                {
                    "title": "Practical Quantum Error Correction Using Surface Codes",
                    "authors": ["Johnson, R.", "Lee, K."],
                    "url": "https://arxiv.org/abs/2602.54321",
                    "published": datetime.now() - timedelta(days=3),
                    "summary": "Experimental validation of surface code implementation on superconducting qubits.",
                    "source": "arXiv",
                    "category": "Quantum Computing"
                }
            ]
        }
        return papers.get(topic, [])


class DataAggregator:
    """æ•¸æ“šèšåˆå™¨"""
    
    def __init__(self):
        self.data_source = MockDataSource()
        self.collected_articles = []
    
    def collect_from_rss_feeds(self, sources: List[str]) -> List[Dict]:
        """å¾ RSS Feeds æ”¶é›†æ•¸æ“š"""
        print(f"\nğŸ“¡ æ­£åœ¨å¾ {len(sources)} å€‹ RSS æºæ”¶é›†æ•¸æ“š...")
        
        all_articles = []
        for source in sources:
            articles = self.data_source.fetch_rss_feed(source)
            all_articles.extend(articles)
            print(f"  âœ“ {source}: æ”¶é›† {len(articles)} ç¯‡æ–‡ç« ")
        
        self.collected_articles.extend(all_articles)
        return all_articles
    
    def collect_from_hacker_news(self) -> List[Dict]:
        """å¾ Hacker News æ”¶é›†æ•¸æ“š"""
        print(f"\nğŸ“¡ æ­£åœ¨å¾ Hacker News æ”¶é›†ç†±é–€è©±é¡Œ...")
        
        articles = self.data_source.fetch_hacker_news()
        self.collected_articles.extend(articles)
        print(f"  âœ“ æ”¶é›† {len(articles)} å€‹ç†±é–€è¨è«–")
        
        return articles
    
    def collect_from_arxiv(self, topics: List[str]) -> List[Dict]:
        """å¾ arXiv æ”¶é›†ç ”ç©¶è«–æ–‡"""
        print(f"\nğŸ“¡ æ­£åœ¨å¾ arXiv æ”¶é›†ç ”ç©¶è«–æ–‡...")
        
        all_papers = []
        for topic in topics:
            papers = self.data_source.fetch_arxiv_papers(topic)
            all_papers.extend(papers)
            print(f"  âœ“ {topic}: æ”¶é›† {len(papers)} ç¯‡è«–æ–‡")
        
        self.collected_articles.extend(all_papers)
        return all_papers
    
    def filter_by_category(self, category: str) -> List[Dict]:
        """æŒ‰åˆ†é¡éæ¿¾"""
        return [
            article for article in self.collected_articles
            if article.get("category") == category
        ]
    
    def filter_by_date(self, hours: int = 24) -> List[Dict]:
        """æŒ‰æ™‚é–“éæ¿¾ï¼ˆæœ€è¿‘ N å°æ™‚ï¼‰"""
        cutoff = datetime.now() - timedelta(hours=hours)
        return [
            article for article in self.collected_articles
            if article.get("published", datetime.min) > cutoff
        ]
    
    def get_top_by_score(self, n: int = 10) -> List[Dict]:
        """ç²å–è©•åˆ†æœ€é«˜çš„æ–‡ç« """
        scored_articles = [
            article for article in self.collected_articles
            if "score" in article
        ]
        return sorted(scored_articles, key=lambda x: x["score"], reverse=True)[:n]
    
    def extract_keywords(self, article: Dict) -> List[str]:
        """æå–é—œéµå­—ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        text = f"{article.get('title', '')} {article.get('summary', '')}"
        
        # ç°¡åŒ–çš„é—œéµå­—æå–ï¼ˆå¯¦éš›æœƒç”¨ NLPï¼‰
        keywords = []
        important_terms = [
            "AI", "machine learning", "quantum", "blockchain", "cloud",
            "security", "data", "automation", "API", "infrastructure",
            "CEO", "leadership", "strategy", "innovation", "digital transformation"
        ]
        
        text_lower = text.lower()
        for term in important_terms:
            if term.lower() in text_lower:
                keywords.append(term)
        
        return keywords
    
    def analyze_trends(self) -> Dict:
        """åˆ†æè¶¨å‹¢"""
        category_counts = {}
        keyword_counts = {}
        sources_counts = {}
        
        for article in self.collected_articles:
            # çµ±è¨ˆåˆ†é¡
            category = article.get("category", "Unknown")
            category_counts[category] = category_counts.get(category, 0) + 1
            
            # çµ±è¨ˆä¾†æº
            source = article.get("source", "Unknown")
            sources_counts[source] = sources_counts.get(source, 0) + 1
            
            # çµ±è¨ˆé—œéµå­—
            keywords = self.extract_keywords(article)
            for kw in keywords:
                keyword_counts[kw] = keyword_counts.get(kw, 0) + 1
        
        return {
            "total_articles": len(self.collected_articles),
            "by_category": category_counts,
            "by_source": sources_counts,
            "top_keywords": sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10],
            "date_range": {
                "oldest": min([a.get("published", datetime.now()) for a in self.collected_articles]).strftime("%Y-%m-%d %H:%M"),
                "newest": max([a.get("published", datetime.now()) for a in self.collected_articles]).strftime("%Y-%m-%d %H:%M")
            }
        }
    
    def generate_summary_for_ceo(self, topic: str = "strategic_intelligence") -> Dict:
        """ç‚º CEO ç”Ÿæˆæ‘˜è¦"""
        
        # æ ¹æ“šä¸»é¡Œéæ¿¾ç›¸é—œæ–‡ç« 
        relevant_categories = {
            "strategic_intelligence": ["Economics", "Markets", "Leadership", "Management"],
            "technology_radar": ["AI/ML", "Technology", "Developer Tools", "Databases"],
            "market_pulse": ["Markets", "Economics", "Fintech"]
        }
        
        target_categories = relevant_categories.get(topic, [])
        relevant_articles = [
            article for article in self.collected_articles
            if article.get("category") in target_categories
        ]
        
        # æœ€è¿‘ 24 å°æ™‚çš„æ–‡ç« 
        recent_articles = [
            article for article in relevant_articles
            if (datetime.now() - article.get("published", datetime.min)).total_seconds() < 86400
        ]
        
        return {
            "topic": topic,
            "total_relevant": len(relevant_articles),
            "recent_24h": len(recent_articles),
            "top_stories": recent_articles[:5],
            "key_themes": self._extract_themes(relevant_articles)
        }
    
    def _extract_themes(self, articles: List[Dict]) -> List[str]:
        """æå–ä¸»é¡Œ"""
        keyword_counts = {}
        for article in articles:
            keywords = self.extract_keywords(article)
            for kw in keywords:
                keyword_counts[kw] = keyword_counts.get(kw, 0) + 1
        
        return [kw for kw, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:5]]


# ============================================================================
# åŸ·è¡Œæ¸¬è©¦
# ============================================================================

def main():
    print("=" * 80)
    print("ğŸ“Š å¸‚å ´æ•¸æ“šæ”¶é›†èˆ‡æ•´åˆç³»çµ±æ¸¬è©¦")
    print("=" * 80)
    
    aggregator = DataAggregator()
    
    # Test 1: RSS Feeds æ”¶é›†
    print("\nâœ… Test 1: å¾ RSS Feeds æ”¶é›†æ•¸æ“š")
    print("-" * 80)
    
    rss_sources = ["TechCrunch", "Bloomberg", "Wired", "Harvard Business Review"]
    rss_articles = aggregator.collect_from_rss_feeds(rss_sources)
    print(f"\nç¸½å…±æ”¶é›†: {len(rss_articles)} ç¯‡æ–‡ç« ")
    
    # Test 2: Hacker News æ”¶é›†
    print("\nâœ… Test 2: å¾ Hacker News æ”¶é›†æ•¸æ“š")
    print("-" * 80)
    
    hn_articles = aggregator.collect_from_hacker_news()
    print(f"\nç¸½å…±æ”¶é›†: {len(hn_articles)} å€‹è¨è«–")
    
    # Test 3: arXiv è«–æ–‡æ”¶é›†
    print("\nâœ… Test 3: å¾ arXiv æ”¶é›†ç ”ç©¶è«–æ–‡")
    print("-" * 80)
    
    arxiv_topics = ["AI", "Quantum"]
    arxiv_papers = aggregator.collect_from_arxiv(arxiv_topics)
    print(f"\nç¸½å…±æ”¶é›†: {len(arxiv_papers)} ç¯‡è«–æ–‡")
    
    # Test 4: è¶¨å‹¢åˆ†æ
    print("\nâœ… Test 4: åˆ†ææ”¶é›†çš„æ•¸æ“šè¶¨å‹¢")
    print("-" * 80)
    
    trends = aggregator.analyze_trends()
    print(f"\nç¸½æ–‡ç« æ•¸: {trends['total_articles']}")
    print(f"æ™‚é–“ç¯„åœ: {trends['date_range']['oldest']} åˆ° {trends['date_range']['newest']}")
    
    print("\nä¾åˆ†é¡åˆ†å¸ƒ:")
    for category, count in sorted(trends['by_category'].items(), key=lambda x: x[1], reverse=True):
        print(f"  - {category}: {count} ç¯‡")
    
    print("\nä¾ä¾†æºåˆ†å¸ƒ:")
    for source, count in sorted(trends['by_source'].items(), key=lambda x: x[1], reverse=True):
        print(f"  - {source}: {count} ç¯‡")
    
    print("\nç†±é–€é—œéµå­—:")
    for i, (keyword, count) in enumerate(trends['top_keywords'], 1):
        print(f"  {i}. {keyword}: å‡ºç¾ {count} æ¬¡")
    
    # Test 5: æŒ‰åˆ†é¡éæ¿¾
    print("\nâœ… Test 5: æŒ‰åˆ†é¡éæ¿¾æ–‡ç« ")
    print("-" * 80)
    
    ai_articles = aggregator.filter_by_category("AI/ML")
    print(f"AI/ML ç›¸é—œæ–‡ç« : {len(ai_articles)} ç¯‡")
    for article in ai_articles:
        print(f"  - {article['title']}")
        print(f"    ä¾†æº: {article['source']}, ç™¼å¸ƒ: {article['published'].strftime('%Y-%m-%d %H:%M')}")
    
    # Test 6: æŒ‰æ™‚é–“éæ¿¾
    print("\nâœ… Test 6: éæ¿¾æœ€è¿‘ 12 å°æ™‚çš„æ–‡ç« ")
    print("-" * 80)
    
    recent_articles = aggregator.filter_by_date(hours=12)
    print(f"æœ€è¿‘ 12 å°æ™‚: {len(recent_articles)} ç¯‡")
    for article in recent_articles:
        hours_ago = (datetime.now() - article['published']).total_seconds() / 3600
        print(f"  - {article['title']}")
        print(f"    {hours_ago:.1f} å°æ™‚å‰ | {article['source']}")
    
    # Test 7: è©•åˆ†æ’åº
    print("\nâœ… Test 7: ç²å–é«˜è©•åˆ†å…§å®¹")
    print("-" * 80)
    
    top_scored = aggregator.get_top_by_score(n=3)
    print(f"è©•åˆ†æœ€é«˜çš„ {len(top_scored)} å€‹è©±é¡Œ:")
    for i, article in enumerate(top_scored, 1):
        print(f"\n  {i}. {article['title']}")
        print(f"     è©•åˆ†: {article['score']} | è©•è«–: {article['comments']} | {article['source']}")
    
    # Test 8: ç‚º CEO ç”Ÿæˆä¸»é¡Œæ‘˜è¦
    print("\nâœ… Test 8: ç‚º CEO ç”Ÿæˆä¸»é¡Œæ‘˜è¦")
    print("-" * 80)
    
    topics_to_test = ["strategic_intelligence", "technology_radar", "market_pulse"]
    
    for topic in topics_to_test:
        print(f"\nğŸ“° ä¸»é¡Œ: {topic.replace('_', ' ').title()}")
        print("-" * 60)
        
        summary = aggregator.generate_summary_for_ceo(topic)
        print(f"ç›¸é—œæ–‡ç« ç¸½æ•¸: {summary['total_relevant']}")
        print(f"æœ€è¿‘ 24 å°æ™‚: {summary['recent_24h']}")
        print(f"é—œéµä¸»é¡Œ: {', '.join(summary['key_themes'])}")
        
        print(f"\næœ€æ–°æ•…äº‹:")
        for i, story in enumerate(summary['top_stories'][:3], 1):
            print(f"  {i}. {story['title']}")
            print(f"     {story['source']} | {story.get('category', 'N/A')}")
    
    # Test 9: é—œéµå­—æå–
    print("\nâœ… Test 9: æ¸¬è©¦é—œéµå­—æå–åŠŸèƒ½")
    print("-" * 80)
    
    sample_article = rss_articles[0]
    keywords = aggregator.extract_keywords(sample_article)
    print(f"\næ–‡ç« : {sample_article['title']}")
    print(f"æå–çš„é—œéµå­—: {', '.join(keywords)}")
    
    # Final Summary
    print("\n" + "=" * 80)
    print("ğŸ“Š æ•¸æ“šæ”¶é›†ç³»çµ±æ¸¬è©¦æ‘˜è¦")
    print("=" * 80)
    
    print(f"""
âœ… RSS Feeds æ”¶é›†: æˆåŠŸï¼ˆ{len(rss_sources)} å€‹ä¾†æºï¼‰
âœ… Hacker News æ”¶é›†: æˆåŠŸ
âœ… arXiv è«–æ–‡æ”¶é›†: æˆåŠŸï¼ˆ{len(arxiv_topics)} å€‹ä¸»é¡Œï¼‰
âœ… æ•¸æ“šéæ¿¾: æˆåŠŸï¼ˆåˆ†é¡ã€æ™‚é–“ã€è©•åˆ†ï¼‰
âœ… è¶¨å‹¢åˆ†æ: æˆåŠŸ
âœ… CEO æ‘˜è¦ç”Ÿæˆ: æˆåŠŸï¼ˆ{len(topics_to_test)} å€‹ä¸»é¡Œï¼‰
âœ… é—œéµå­—æå–: æˆåŠŸ

ç¸½æ”¶é›†æ–‡ç« : {trends['total_articles']} ç¯‡
æ•¸æ“šä¾†æº: {len(trends['by_source'])} å€‹
è¦†è“‹åˆ†é¡: {len(trends['by_category'])} å€‹
ç†±é–€é—œéµå­—: {len(trends['top_keywords'])} å€‹

âœ… æ‰€æœ‰æ•¸æ“šæ”¶é›†åŠŸèƒ½æ¸¬è©¦é€šéï¼
ç³»çµ±å·²æº–å‚™å¥½ç‚º CEO Newsletter æä¾›å¸‚å ´æƒ…å ±ã€‚
""")
    
    print("=" * 80)

if __name__ == "__main__":
    main()
